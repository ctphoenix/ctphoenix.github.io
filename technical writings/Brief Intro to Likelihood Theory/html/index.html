<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>A Brief Intro to Likelihood Theory - Patrick Staples</title>
<link href="../../../styles/main.css" rel="stylesheet"/>
<!-- MathJax for rendering LaTeX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div class="top-container">
<div class="content-container">
<header class="site-header">
<h1 class="brand">Patrick Staples</h1>
</header>
<nav>
    <div class="nav-content">
      <span class="nav-item">
        <a href="../../../index.html">Home</a>
      </span>
      <span class="nav-item active">
        <a href="../../../technical-writings.html">Technical Writings</a>
      </span>
      <span class="nav-item">
        <a href="../../../music.html">Music</a>
      </span>
      <span class="nav-item">
        <a href="https://www.linkedin.com/in/patrick-staples/" target="_blank">LinkedIn</a>
      </span>
      <span class="nav-item">
        <a href="https://scholar.google.com/citations?user=obAll0UAAAAJ" target="_blank">Google Scholar</a>
      </span>
    </div>
  </nav>
<main>
<div class="breadcrumbs">
    <a href="../../../technical-writings.html">← Back to Technical Writings</a>
</div>
<h1>A Brief Intro to Likelihood Theory</h1>
<div class="math-content">
<p>This is a brief introduction to likelihood theory. It's a powerful tool for statistical inference, and it's the foundation for many statistical methods.</p>

<h2>What is Likelihood?</h2>
<p>The likelihood function \(L(\theta | x)\) is proportional to the probability of observing data \(x\) given parameters \(\theta\):</p>

<p>\[L(\theta | x) \propto P(x | \theta)\]</p>

<p>For discrete random variables, the likelihood is exactly the probability mass function evaluated at the observed data. For continuous random variables, it's the probability density function.</p>

<h2>The Likelihood Principle</h2>
<p>The likelihood principle states that all the information about \(\theta\) in a sample is contained in the likelihood function. This means that two different statistical models or experiments that yield the same likelihood function should lead to the same inferences about \(\theta\).</p>

<h2>Maximum Likelihood Estimation</h2>
<p>The maximum likelihood estimate (MLE) of \(\theta\) is the value that maximizes the likelihood function:</p>

<p>\[\hat{\theta}_{MLE} = \arg\max_{\theta} L(\theta | x)\]</p>

<p>Often, we work with the log-likelihood function \(\ell(\theta | x) = \log L(\theta | x)\) because it's mathematically more convenient and doesn't change the location of the maximum.</p>

<h2>Example: Normal Distribution</h2>
<p>For a sample \(x_1, x_2, \ldots, x_n\) from a normal distribution \(N(\mu, \sigma^2)\), the likelihood function is:</p>

<p>\[L(\mu, \sigma^2 | x) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right)\]</p>

<p>Taking the log:</p>

<p>\[\ell(\mu, \sigma^2 | x) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_i - \mu)^2\]

<p>The MLEs are:</p>

<p>\[\hat{\mu} = \frac{1}{n}\sum_{i=1}^{n}x_i\]</p>

<p>\[\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n}(x_i - \hat{\mu})^2\]</p>

<h2>Likelihood Ratio Tests</h2>
<p>The likelihood ratio test compares the maximum likelihood under a restricted model (null hypothesis \(H_0\)) to the maximum likelihood under an unrestricted model (alternative hypothesis \(H_1\)):</p>

<p>\[\Lambda = \frac{\max_{\theta \in \Theta_0} L(\theta | x)}{\max_{\theta \in \Theta} L(\theta | x)}\]</p>

<p>Under certain regularity conditions, \(-2\log\Lambda\) follows a chi-squared distribution asymptotically.</p>

<h2>Visualization</h2>
<p>Here's a visualization of a likelihood function for a simple parameter:</p>

<figure class="figure">
  <img src="likelihood_plot.png" alt="Likelihood function plot" style="width:70%; margin:auto; display:block;">
  <figcaption>Likelihood function for a parameter θ. The maximum likelihood estimate is where the function peaks.</figcaption>
</figure>

<h2>Conclusion</h2>
<p>Likelihood theory provides a unified framework for statistical inference. It's the basis for many statistical methods, including maximum likelihood estimation, likelihood ratio tests, and Bayesian inference (where the likelihood is combined with a prior distribution).</p>
</div>
<div class="breadcrumbs">
    <a href="../../../technical-writings.html">← Back to Technical Writings</a>
</div>
</main>
</div>
</div>
</body>
</html> 